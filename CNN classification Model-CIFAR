# Importing necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Dense, LSTM, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from sklearn.metrics import mean_absolute_error, mean_squared_error

# Step 1: Data Collection and Preprocessing
# Clone the NAB dataset repository
!git clone https://github.com/numenta/NAB.git
# Navigate to the NAB data folder
!ls NAB/data
# Load a specific dataset
data_path = "NAB/data/artificialWithAnomaly/art_daily_jumpsup.csv"
data = pd.read_csv(data_path)

# Display the first few rows to understand the data structure
print(data.head())

# Convert the timestamp to datetime format
data['timestamp'] = pd.to_datetime(data['timestamp'])
data.set_index('timestamp', inplace=True)

# Extract the value column for analysis
values = data['value'].values.reshape(-1, 1)

# Handle missing data (if any)
data = data.dropna()

# Plot the time series data to understand its patterns
plt.figure(figsize=(10, 6))
plt.plot(data['value'], label="Time Series Data")
plt.xlabel("Time")
plt.ylabel("Value")
plt.title("Time Series Data Visualization")
plt.legend()
plt.show()

# Step 2: Data Preprocessing
# Initialize the scaler
scaler = MinMaxScaler()

# Normalize (scale) the 'value' column
normalized_values = scaler.fit_transform(values)

# Step : Create lag features for RNN input
sequence_length = 30  # Number of previous timesteps to use for prediction
X, y = [], []
for i in range(sequence_length, len(normalized_values)):
    X.append(normalized_values[i-sequence_length:i])
    y.append(normalized_values[i])

X, y = np.array(X), np.array(y)

# Split the data into training and testing sets
split_index = int(len(X) * 0.8)
X_train, X_test = X[:split_index], X[split_index:]
y_train, y_test = y[:split_index], y[split_index:]

# Step 3: Build the RNN model
model = Sequential([
    LSTM(65, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),
    Dense(1)
])

# Compile the model
model.compile(optimizer='adam', loss='mse')

# Add callbacks for early stopping and model checkpointing
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
checkpoint = ModelCheckpoint('best_rnn_model.h5', monitor='val_loss', save_best_only=True)

# Step 4: Train the model
history = model.fit(X_train, y_train, epochs=20, batch_size=32,
                    validation_data=(X_test, y_test),
                    callbacks=[early_stopping, checkpoint], verbose=1)

# Step 5: Evaluate the model
train_loss = model.evaluate(X_train, y_train, verbose=0)
test_loss = model.evaluate(X_test, y_test, verbose=0)
print(f"Training Loss: {train_loss:.4f}")
print(f"Testing Loss: {test_loss:.4f}")

# Step 6: Forecast future values
predicted = model.predict(X_test)

# Invert normalization for actual and predicted values
y_test_original = scaler.inverse_transform(y_test.reshape(-1, 1))
predicted_original = scaler.inverse_transform(predicted)

# Step 7: Visualization of Results
# Plot actual vs predicted values
plt.figure(figsize=(10, 6))
plt.plot(y_test_original, label='Actual Values')
plt.plot(predicted_original, label='Predicted Values')
plt.xlabel('Time Steps')
plt.ylabel('Value')
plt.title('Forecasting with LSTM')
plt.legend()
plt.show()

# Plot the loss curves
plt.figure(figsize=(9, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Loss Curves')
plt.legend()
plt.show()

# Step 8: Evaluate Metrics
def evaluate_metrics(y_true, y_pred):
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mse = mean_squared_error(y_true, y_pred)
    return mae, rmse, mse

mae, rmse, mse = evaluate_metrics(y_test_original, predicted_original)
print(f"MAE: {mae:.4f}, RMSE: {rmse:.4f}, MSE: {mse:.4f}")

# Save the model
#model.save('simplernn_forecasting_model.h5')
